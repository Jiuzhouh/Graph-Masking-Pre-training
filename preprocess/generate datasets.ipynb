{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import ast\n",
    "import json\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_dataset(origin_src, origin_tgt, cleaned_src, cleaned_tgt):\n",
    "    xy_train = pd.read_table(origin_src, header = None)\n",
    "    xy_train_target = pd.read_table(origin_tgt, header = None)\n",
    "    \n",
    "    xy_train_both = pd.concat((xy_train, xy_train_target), axis=1)\n",
    "    xy_train_both_new = xy_train_both.drop_duplicates()\n",
    "    xy_train_both_new = xy_train_both_new.reset_index()\n",
    "    del xy_train_both_new['index']\n",
    "    xy_train_both_new.columns=[1,2]\n",
    "    \n",
    "    xy_train_src = np.split(xy_train_both_new, [1], axis=1)[0]\n",
    "    xy_train_tgt = np.split(xy_train_both_new, [1], axis=1)[1]\n",
    "    \n",
    "    with open(cleaned_src, 'a') as f1:\n",
    "        for i in range(len(xy_train_src)):\n",
    "            f1.write(xy_train_src[1][i] + '\\n')\n",
    "        \n",
    "    with open(cleaned_tgt, 'a') as f2:\n",
    "        for i in range(len(xy_train_tgt)):\n",
    "            f2.write(xy_train_tgt[2][i] + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_triple(pair_src, pair_tgt, l, remove = False):\n",
    "    wf_src = open(pair_src, 'a')\n",
    "    wf_tgt = open(pair_tgt, 'a')\n",
    "    \n",
    "    tag = 1\n",
    "    tag2 = 1\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        replace_token = ['<X>']\n",
    "        y = '<X> '\n",
    "        x = copy.deepcopy(l)\n",
    "        \n",
    "        if len(l) != 1:\n",
    "            if x[i][-1] == 1 and tag != 0:\n",
    "                tag -= 1\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                if remove:\n",
    "                    # remove triples without connections\n",
    "                    for j in x_:\n",
    "                        if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                            x.remove(j)\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')\n",
    "            elif x[i][-1] == 2 and tag2 != 0:\n",
    "                tag2 -= 1\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                if remove:\n",
    "                    # remove triples without connections\n",
    "                    for j in x_:\n",
    "                        if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                            x.remove(j)\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')        \n",
    "            elif x[i][-1] == 3:\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                if remove:\n",
    "                    # remove triples without connections\n",
    "                    for j in x_:\n",
    "                        if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                            x.remove(j)\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')        \n",
    "            elif x[i][-1] == 4:\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                if remove:\n",
    "                    # remove triples without connections\n",
    "                    for j in x_:\n",
    "                        if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                            x.remove(j)\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')        \n",
    "\n",
    "    wf_tgt.close()\n",
    "    wf_src.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_relation(pair_src, pair_tgt, l):\n",
    "    wf_src = open(pair_src, 'a')\n",
    "    wf_tgt = open(pair_tgt, 'a')\n",
    "\n",
    "    for i in range(len(l)):\n",
    "        replace_token2 = '<Y>'\n",
    "        x = copy.deepcopy(l)\n",
    "        x[i][1] = replace_token2\n",
    "        y = '<Y> ' + l[i][1] +' <Z>'\n",
    "        wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "        wf_tgt.write(y + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_triple_and_relation(pair_src, pair_tgt, l):\n",
    "    wf_src = open(pair_src, 'a')\n",
    "    wf_tgt = open(pair_tgt, 'a')\n",
    "    \n",
    "    tag = 1\n",
    "    tag2 = 1\n",
    "    \n",
    "    for i in range(len(l)):\n",
    "        replace_token1 = ['<X>']\n",
    "        replace_token2 = '<Y>'\n",
    "        y = '<X> '\n",
    "        x = copy.deepcopy(l)  \n",
    "        \n",
    "        if len(l) != 1:\n",
    "            if x[i][-1] == 1 and tag !=0:\n",
    "                tag -= 1\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token1 + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                for j in x_:\n",
    "                    if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                        y = y[:-3] + '<Y> ' + x[x.index(j)][1] +'<Z>'\n",
    "                        x[x.index(j)][1] = replace_token2\n",
    "                        break\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')\n",
    "\n",
    "            elif x[i][-1] == 2 and tag2 !=0:\n",
    "                tag2 -= 1\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token1 + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                for j in x_:\n",
    "                    if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                        index = x.index(j)\n",
    "                        y = y[:-3] + '<Y> ' + x[index][1] +'<Z>'       \n",
    "                        x[index][1] = replace_token2\n",
    "                        break\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')\n",
    "\n",
    "            elif x[i][-1] == 3:\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token1 + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                for j in x_:\n",
    "                    if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                        y = y[:-3] + '<Y> ' + x[x.index(j)][1] +'<Z>'\n",
    "                        x[x.index(j)][1] = replace_token2\n",
    "                        break\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')\n",
    "\n",
    "            elif x[i][-1] == 4:\n",
    "                y += str(x[i][:-1])+' <Z>'\n",
    "                x[i] = replace_token1 + x[i][-1:]\n",
    "                x_ = x[:]\n",
    "                for j in x_:\n",
    "                    if '<X>' not in j and 'S| '+l[i][0][3:] not in j and 'O| '+l[i][0][3:] not in j and 'S| '+l[i][2][3:] not in j and 'O| '+l[i][2][3:] not in j:\n",
    "                        y = y[:-3] + '<Y> ' + x[x.index(j)][1] +'<Z>'\n",
    "                        x[x.index(j)][1] = replace_token2\n",
    "                        break\n",
    "                wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "                wf_tgt.write(y + '\\n')\n",
    "                \n",
    "        else:\n",
    "            y = '<Y> ' + x[i][1] +'<Z>'\n",
    "            x[i][1] = replace_token2\n",
    "            wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "            wf_tgt.write(y + '\\n')\n",
    "\n",
    "    wf_tgt.close()\n",
    "    wf_src.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_triple_eventNar(pair_src, pair_tgt, l):\n",
    "    wf_src = open(pair_src, 'a')\n",
    "    wf_tgt = open(pair_tgt, 'a')\n",
    "    \n",
    "    if len(l) != 1:\n",
    "        x = copy.deepcopy(l)\n",
    "        y = ''\n",
    "        replace_token1 = '<X>'\n",
    "        y = ''\n",
    "        x[0] = replace_token1\n",
    "        y += replace_token1 + ' ' + str(l[0]) + ' '\n",
    "        y += '<Z>'\n",
    "        wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "        wf_tgt.write(y + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_dataset_triple_and_relation_eventNar(pair_src, pair_tgt, l):\n",
    "    wf_src = open(pair_src, 'a')\n",
    "    wf_tgt = open(pair_tgt, 'a')\n",
    "    \n",
    "    x = copy.deepcopy(l)\n",
    "    replace_token1 = '<X>'\n",
    "    replace_token2 = '<Y>'\n",
    "    if len(l) != 1:\n",
    "        y = ''\n",
    "        x[0] = replace_token1\n",
    "        y += replace_token1 + ' ' + str(l[0]) + ' '\n",
    "        x[-1][1] = replace_token2\n",
    "        y += replace_token2 + ' ' + l[-1][1] + ' '\n",
    "    else:\n",
    "        x[0][1] = replace_token2\n",
    "        y = replace_token2 + ' ' + l[0][1] + ' '\n",
    "    y += '<Z>'\n",
    "    wf_src.write(json.dumps(x, ensure_ascii=False) + '\\n')\n",
    "    wf_tgt.write(y + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_train_src1 = \"masked_webnlg20/train_triple.source\"\n",
    "pair_train_tgt1 = \"masked_webnlg20/train_triple.target\"\n",
    "\n",
    "pair_train_src2 = \"masked_webnlg20/train_triple_and_relation.source\"\n",
    "pair_train_tgt2 = \"masked_webnlg20/train_triple_and_relation.target\"\n",
    "\n",
    "with open(\"webnlg20_pos_ordered/train.source\") as f:\n",
    "    for l in f.readlines():\n",
    "        # change string to list\n",
    "        l = ast.literal_eval(l)\n",
    "        generate_dataset_triple(pair_train_src1, pair_train_tgt1, l, remove = False)\n",
    "        generate_dataset_triple_and_relation(pair_train_src2, pair_train_tgt2, l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_origin_src = \"masked_webnlg20/triple_and_relation/train.source\"\n",
    "train_origin_tgt = \"masked_webnlg20/triple_and_relation/train.target\"\n",
    "train_cleaned_src = \"masked_webnlg20/triple_and_relation_cleaned/train.source\"\n",
    "train_cleaned_tgt = \"masked_webnlg20/triple_and_relation_cleaned/train.target\"\n",
    "clean_dataset(train_origin_src, train_origin_tgt, train_cleaned_src, train_cleaned_tgt)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
